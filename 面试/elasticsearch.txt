Elasticsearch:
          是一个分布式,高性能，高可用，可伸缩的搜索与分析系统
          lucene是一个jar包，里面包含封装好的各种建立倒排索引,以及进行搜索的代码,包含各种算法

          功能:
                1.分布式搜索引擎和数据分析引擎
                2.全文检索,结构化检索,数据分析
                3.海量数据实时处理    es可以将海量数据分散到各个服务器进行存储与检索，提供近实时的查询功能
          特点:
                1.可搭建集群用于大公司,也可以单机运行用于小公司
                2.将全文检索,数据分析，与分布式集于一生
                3. 开箱即用,几分钟部署es,
                4.作为数据库功能不足,如事务,但又有特殊的功能,全文检索，同义词处理，相关度排名
         核心概念:
                 cluster  集群  包含多个节点，每个节点属于哪个集群需要配置
                 node   节点集群中提供服务的机器
                 document&field 文档 es中最小的数据单元，一个document可以是一条客户数据,用json数据结构表示，每个index下的type可以存储多个document，一个document里面又有多个field，每个field就是一个数据字段
                inde 索引   一堆又像是结构的文档数据
                type  类型 一个索引的逻辑数据分类
                shard  es将每个索引切分为多个shard，分布在服务器上面存储。有了shard可以横向扩展，提高吞吐量与性能
                replication shard的副本

index 相当于数据库
type  相当于表
document 相当于行

分布式实现
分布式算法
shard = hash(routing) % number_of_primary_shards
以上是路由文档到分片的算法，routing值是一个任意字符串，它默认是_id但也可以自定义。这个routing字符串通过哈希函数生成一个数字，然后除以主切片的数量得到一个余数(remainder)，余数的范围永远是0到number_of_primary_shards - 1，这个数字就是特定文档所在的分片。
这也解释了为什么主分片的数量只能在创建索引时定义且不能修改：如果主分片的数量在未来改变了，所有先前的路由值就失效了，文档也就永远找不到了。

写请求
新建、索引和删除请求都是写(write)操作，它们必须在主分片上成功完成才能复制到相关的复制分片上。
下面我们罗列在主分片和复制分片上成功新建、索引或删除一个文档必要的顺序步骤：
客户端给Node 1发送新建、索引或删除请求。
节点使用文档的_id确定文档属于分片0。它转发请求到Node 3，分片0位于这个节点上。
Node 3在主分片上执行请求，如果成功，它转发请求到相应的位于Node 1和Node 2的复制节点上。当所有的复制节点报告成功，Node 3报告成功到请求的节点，请求的节点再报告给客户端。
个人理解：此处每个节点都知道index_X-shard_N-node_M的元数据信息，才能进行相应的请求转发。

读请求
下面我们罗列在主分片或复制分片上检索一个文档必要的顺序步骤：
客户端给Node 1发送get请求。
节点使用文档的_id确定文档属于分片0。分片0对应的复制分片在三个节点上都有。此时，它转发请求到Node 2。
Node 2返回endangered给Node 1然后返回给客户端。

局部更新
update?API 结合了之前提到的读和写的模式。
下面我们罗列执行局部更新必要的顺序步骤
客户端给Node 1发送更新请求。
它转发请求到主分片所在节点Node 3。
Node 3从主分片检索出文档，修改_source字段的JSON，然后在主分片上重建索引。如果有其他进程修改了文档，它以retry_on_conflict设置的次数重复步骤3，都未成功则放弃。
如果Node 3成功更新文档，它同时转发文档的新版本到Node 1和Node 2上的复制节点以重建索引。当所有复制节点报告成功，Node 3返回成功给请求节点，然后返回给客户端

倒排索引不可变
写入磁盘的倒排索引是不可变的，它有如下好处：
不需要锁。如果从来不需要更新一个索引，就不必担心多个程序同时尝试修改。
一旦索引被读入文件系统的缓存(译者:在内存)，它就一直在那儿，因为不会改变。只要文件系统缓存有足够的空间，大部分的读会直接访问内存而
不是磁盘。这有助于性能提升。
在索引的声明周期内，所有的其他缓存都可用。它们不需要在每次数据变化了都重建，因为数据不会变。
写入单个大的倒排索引，可以压缩数据，较少磁盘IO和需要缓存索引的内存大小。
当然，不可变的索引有它的缺点，首先是它不可变！你不能改变它。如果想要搜索一个新文档，必须重见整个索引。这不仅严重限制了一个索引所能
装下的数据，还有一个索引可以被更新的频次。

动态索引
索引不可变首先需要解决的问题是如何在保持不可变好处的同时更新倒排索引。答案是，使用多个索引。不是重写整个倒排索引，而是增加额外的
索引反映最近的变化。每个倒排索引都可以按顺序查询，从最老的开始，最后把结果聚合。

Elasticsearch底层依赖的Lucene，引入了per-segment search的概念。一个段(segment)是有完整功能的倒排索引，但是现在Lucene中的索引
指的是段的集合，再加上提交点(commit point，包括所有段的文件)，如图1所示。新的文档，在被写入磁盘的段之前，首先写入内存区的索引缓存
，如图2、图3所示。

近实时搜索（refresh）
位于Elasticsearch和磁盘间的是文件系统缓存。如前所说，在内存索引缓存中的文档（图1）被写入新的段（图2），但是新的段首先写入文件系
统缓存，这代价很低，之后会被同步到磁盘，这个代价很大。但是一旦一个文件被缓存，它也可以被打开和读取，就像其他文件一样。
 Lucene允许新段写入打开，好让它们包括的文档可搜索，而不用执行一次全量提交。这是比提交更轻量的过程，可以经常操作，而不会影响性能。
在Elesticsearch中，这种写入打开一个新段的轻量级过程，叫做refresh。默认情况下，每个分片每秒自动刷新一次。这就是为什么说Elasticsearch
是近实时的搜索了：文档的改动不会立即被搜索，但是会在一秒内可见。

持久化变更（flush）
没用fsync同步文件系统缓存到磁盘，我们不能确保电源失效，甚至正常退出应用后，数据的安全。为了ES的可靠性，需要确保变更持久化到磁盘。
我们说过一次全提交同步段到磁盘，写提交点，这会列出所有的已知的段。在重启，或重新打开索引时，ES使用这次提交点决定哪些段属于当前的分片。
当我们通过每秒的刷新获得近实时的搜索，我们依然需要定时地执行全提交确保能从失败中恢复。但是提交之间的文档怎么办？我们也不想丢失它们。
ES增加了事务日志（translog），来记录每次操作。有了事务日志，过程现在如下：
当一个文档被索引，它被加入到内存缓存，同时加到事务日志。
图1：新的文档加入到内存缓存，同时写入事务日志
refresh使得分片的进入如下图描述的状态。每秒分片都进行refeash：
内存缓冲区的文档写入到段中，但没有fsync。
段被打开，使得新的文档可以搜索。
缓存被清除
随着更多的文档加入到缓存区，写入日志，这个过程会继续
不时地，比如日志很大了，新的日志会创建，会进行一次全提交：
内存缓存区的所有文档会写入到新段中。
清除缓存
一个提交点写入硬盘
文件系统缓存通过fsync操作flush到硬盘
事务日志被清除
事务日志记录了没有flush到硬盘的所有操作。当故障重启后，ES会用最近一次提交点从硬盘恢复所有已知的段，并且从日志里恢复所有的操作。

端合并（optimize）
通过每秒自动刷新创建新的段，用不了多久段的数量就爆炸了。有太多的段是一个问题。每个段消费文件句柄，内存，cpu资源。更重要的是，每次搜索请求都需要依次检查每个段。段越多，查询越慢。
ES通过后台合并段解决这个问题。小段被合并成大段，再合并成更大的段。
这是旧的文档文件系统删除的时候。旧的段不会再复制到更大的新段中。

这个过程你不必做什么。当你在索引和搜索时ES会自动处理。这个过程如图：两个提交的段和一个未提交的段合并为了一个更大的段所示：
索引过程中，refresh会创建新的段，并打开它。
合并过程会在后台选择一些小的段合并成大的段，这个过程不会中断索引和搜索。
下图描述了合并后的操作：
新的段flush到了硬盘。


新的提交点写入新的段，排除旧的段。
新的段打开供搜索。
旧的段被删除。
合并大的段会消耗很多IO和CPU，如果不检查会影响到搜素性能。默认情况下，ES会限制合并过程，这样搜索就可以有足够的资源进行。

源码分析:
首先从main开始，从start脚本可以看出，从org.elasticsearch.bootstrap.ElasticSearch开始运行。





查询语法:
get  /_cat/{healthy，indices,}?v
创建索引: put /index
删除索引：delete /index
添加数据:   put/index/type/id{"json数据"}
查询数据:   get /index/type/id
修改数据：post  /index/type/id/_update{"doc":{}}
删除数据:  delete  /index/type/id

查询的DSL语法:
GET /ecommerce/product/_search
{
    "query" : {#查询
        "bool" : {
            "must" : {#bool组合查询
                "match" : {
                    "name" : "yagao"
                }
            },
            "filter" : {#过滤
                "range" : {
                    "price" : { "gt" : 25 }
                }
            }
        }
    },
    "sort": [
        { "price": "desc" }
    ], #排序
    "from": 1,
    "size": 1, #分页
    "_source":["name","price"] #指定显示
}

全文检索
GET /ecommerce/product/_search
{
    "query" : {
        "match" : {
            "producer" : "yagao producer"
        }
    }
}
phrase search（短语搜索）  #必须完全包含
GET /ecommerce/product/_search
{
    "query" : {
        "match_phrase" : {
            "producer" : "yagao producer"
        }
    }
}

统计分析:
GET /ecommerce/product/_search
{
  "size": 0,
   "query": {#查询
         "match": {
         "name": "yagao"
        }
      },
   "aggs": {#分组
    "group_by_tags": {
      "terms": { "field": "tags", "order": { "avg_price": "desc" } #排序 },
          "aggs" : {#统计
                "avg_price" : {
                    "avg" : { "field" : "price" }
                }
            }
    }
  }
}

elasticsearch的分布式架构:
       elasticsearch隐藏了复杂的分布式机制： 分片机制， cluster discovery 集群发现机制 ， shard  负载均衡机制,shard 副本,请求路由，集群扩容，shard重新分配
       特别注意增减节点时候数据的reblance 保证负载均衡
       master节点  创建删除索引   增加或删除节点
       节点平等的分布式架构:
               节点对等,每个节点都能接受所有的请求
               自动请求路由
               响应收集
       shard与repication机制
             index包含多个shard,每个shard都是最小的工作单元，承载部分数据,lucenue实例，完整的建立索引与处理请求得能力，增减节点,shard会在nodes中负载均衡,primary shard和replica shard，每个document肯定只存在于某一个primary shard以及其对应的replica shard中，不可能存在于多个primary shard，replica shard是primary shard的副本，负责容错，以及承担读请求负载，primary shard的数量在创建索引的时候就固定了，replica shard的数量可以随时修改,primary shard的默认数量是5，replica默认是1，默认有10个shard，5个primary shard，5个replica shard，primary shard不能和自己的replica shard放在同一个节点上（否则节点宕机，primary shard和副本都丢失，起不到容错的作用），但是可以和其他primary shard的replica shard放在同一个节点上

11
